{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kdpbiiitd/CSE508_Winter2023_A2_39./blob/main/IR_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwb1wKUCDXmW",
        "outputId": "f3578131-ce9c-461a-f807-0c2d91a7805f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import math\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6hdg3htdBrn",
        "outputId": "49431af5-f6a1-4c2e-a3ca-d600e9f27b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text  Category\n",
            "0  worldcom ex-boss launches defence lawyers defe...  business\n",
            "1  german business confidence slides german busin...  business\n",
            "2  bbc poll indicates economic gloom citizens in ...  business\n",
            "3  lifestyle  governs mobile choice  faster  bett...      tech\n",
            "4  enron bosses in $168m payout eighteen former e...  business\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('dataset.csv')\n",
        "df = df.drop('ArticleId', axis=1)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wlKYrcTshwOZ"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = \" \".join([word.lower() for word in text.split() if word.lower() not in stopwords])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "nELC8mSoh4Pu"
      },
      "outputs": [],
      "source": [
        "df['Text'] = df['Text'].apply(clean_text)\n",
        "df['Text'] = df['Text'].apply(word_tokenize)\n",
        "df['Text'] = df['Text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "df['Text'] = df['Text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate TF\n",
        "tf = defaultdict(Counter)\n",
        "for i, row in df.iterrows():\n",
        "    for word in row['Text']:\n",
        "        tf[row['Category']][word] += 1\n",
        "\n",
        "# Calculate ICF\n",
        "N = len(df)\n",
        "icf = {}\n",
        "for word in set(df['Text'].sum()):\n",
        "    n_t = sum([1 for i, row in df.iterrows() if word in row['Text']])\n",
        "    icf[word] = math.log(N / n_t)\n",
        "\n",
        "# Calculate TF-ICF\n",
        "tf_icf = defaultdict(dict)\n",
        "for label, counter in tf.items():\n",
        "    for word, count in counter.items():\n",
        "        tf_icf[label][word] = count * icf[word]"
      ],
      "metadata": {
        "id": "dBHxobrEn2If"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Calculate the prior probabilities of each category in the training set\n",
        "prior_probs = defaultdict(float)\n",
        "total_count = len(train_df)\n",
        "for category, count in train_df['Category'].value_counts().items():\n",
        "    prior_probs[category] = count / total_count\n",
        "\n",
        "# Calculate the likelihood probabilities of each word in each category in the training set\n",
        "tf = defaultdict(Counter)\n",
        "for i, row in train_df.iterrows():\n",
        "    for word in row['Text']:\n",
        "        tf[row['Category']][word] += 1\n",
        "\n",
        "N = len(train_df)\n",
        "icf = defaultdict(float)\n",
        "for word in set(train_df['Text'].sum()):\n",
        "    n_t = sum([1 for i, row in train_df.iterrows() if word in row['Text']])\n",
        "    icf[word] = math.log(N / n_t)\n",
        "\n",
        "tf_icf = defaultdict(dict)\n",
        "for label, counter in tf.items():\n",
        "    for word, count in counter.items():\n",
        "        tf_icf[label][word] = count * icf[word]"
      ],
      "metadata": {
        "id": "vzs_Ta0MdXuf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store the likelihood probabilities\n",
        "likelihood_probs = defaultdict(dict)\n",
        "for label, counter in tf_icf.items():\n",
        "    total_count = sum(counter.values())\n",
        "    for word, count in counter.items():\n",
        "        likelihood_probs[label][word] = count / total_count"
      ],
      "metadata": {
        "id": "J_1szneNgu93"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Naive Bayes algorithm to predict the category of each text in the test set\n",
        "y_true = test_df['Category'].tolist()\n",
        "y_pred = []\n",
        "for i, row in test_df.iterrows():\n",
        "    probs = defaultdict(float)\n",
        "    for label in prior_probs.keys():\n",
        "        probs[label] = math.log(prior_probs[label])\n",
        "        for word in row['Text']:\n",
        "            if word in likelihood_probs[label]:\n",
        "                probs[label] += math.log(likelihood_probs[label][word])\n",
        "            else:\n",
        "                probs[label] += math.log(1e-9)\n",
        "    y_pred.append(max(probs, key=probs.get))"
      ],
      "metadata": {
        "id": "VdlxcAWOgyuC"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate performance\n",
        "print('Accuracy: {}'.format(accuracy_score(y_true, y_pred)))\n",
        "print('Precision: {}'.format(precision_score(y_true, y_pred, average='weighted')))\n",
        "print('Recall: {}'.format(recall_score(y_true, y_pred, average='weighted')))\n",
        "print('F1: {}'.format(f1_score(y_true, y_pred, average='weighted')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PkUJDDBhBu3",
        "outputId": "acf53307-417b-4aaa-b606-790b22dffcfa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9888143176733781\n",
            "Precision: 0.9890153235437588\n",
            "Recall: 0.9888143176733781\n",
            "F1: 0.9888031097867478\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyuqkA8FdDwFWN1E9PvNyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}